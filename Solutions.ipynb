{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "fb15f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to install \"gprof2dot\"\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import pydot\n",
    "\n",
    "eps = 1e-5  # a small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "f4281459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 Decision Trees\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    # A method that takes in some feature of the data, the labels and a threshold, and compute \n",
    "    # the information gain of a split using the threshold.\n",
    "    def information_gain(X, y, thresh):\n",
    "        # A method that takes in the labels of data stored at a node and compute the entropy \n",
    "        def gini_impurity(X, y):  \n",
    "            d = dict()\n",
    "            for i in range(len(X)):\n",
    "                if y[i] in d:\n",
    "                    d[y[i]] += 1\n",
    "                else:\n",
    "                    d[y[i]] = 1\n",
    "\n",
    "            entropy = 0\n",
    "            for key in d:\n",
    "                p_c = d[key]/len(X)\n",
    "                entropy -= p_c * math.log2(p_c)\n",
    "            return entropy\n",
    "    \n",
    "        # Compute current entropy\n",
    "        H_S = gini_impurity(X, y)\n",
    "        \n",
    "        # Compute entropy after splitting by threshold.\n",
    "        left_X, left_y = [], []\n",
    "        right_X, right_y = [], []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] < thresh:\n",
    "                left_X.append(X[i])\n",
    "                left_y.append(y[i])\n",
    "            else:\n",
    "                right_X.append(X[i])\n",
    "                right_y.append(y[i])\n",
    "                \n",
    "        # Compute information gain        \n",
    "        H_l = gini_impurity(left_X, left_y)\n",
    "        H_r = gini_impurity(right_X, right_y)\n",
    "        H_after = (H_l * len(left_X) + H_r * len(right_X)) / len(X)\n",
    "        return H_S - H_after\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    # Grows a decision tree by constructing nodes. Using the entropy and segmenter methods, it attempts \n",
    "    # to find a configuration of nodes that best splits the input data. This function figures out the split \n",
    "    # rules that each node should have and figures out when to stop growing the tree and insert a leaf node. \n",
    "    # There are many ways to implement this, but eventually your DecisionTree should store the root node \n",
    "    # of the resulting tree so you can use the tree for classification later on. Since the height of your \n",
    "    # DecisionTree shouldn’t be astronomically large (you may want to cap the height—if you do, the max height \n",
    "    # would be a hyperparameter), this method is best implemented recursively.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node splits.\n",
    "            \n",
    "            thresh = np.array([np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10) for i in range(X.shape[1])])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "                \n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            \n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    # Given a data point, traverse the tree to find the best label to classify the data point as. Start at \n",
    "    # the root node you stored and evaluate split rules at each node as you traverse until you reach a leaf node, \n",
    "    # then choose that leaf node’s label as your output label.\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "1a8a7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Random Forests\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, random_states, max_depth, feature_labels):\n",
    "        self.random_states = random_states\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_labels = feature_labels\n",
    "        \n",
    "    def fit_and_predict(self, training_X, training_y, test_X):\n",
    "        predictions = [0] * len(test_X)\n",
    "        for i in range(self.random_states):\n",
    "            dt_titanic = DecisionTree(max_depth=3, feature_labels=features)\n",
    "            rand_X = []\n",
    "            rand_y = []\n",
    "            for i in range(len(training_X)):\n",
    "                randIndex = np.random.randint(len(training_X))\n",
    "                rand_X.append(training_X[randIndex])\n",
    "                rand_y.append(training_y[randIndex])\n",
    "            dt = DecisionTree(self.max_depth, self.feature_labels)\n",
    "            dt.fit(np.array(rand_X), np.array(rand_y))\n",
    "            predictions += dt.predict(test_X)\n",
    "        predictions *= 1/self.random_states\n",
    "        return np.around(predictions)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "361b7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.3 Preprocessing\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == ''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == '-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "571a8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Titanic Dataset\n",
    "\n",
    "# Returns indices for training_labels and training_data that are set aside for validation and for training. Returns\n",
    "# validation and training indices.\n",
    "def shuffle_partition(data, count, total):\n",
    "    np.random.seed(189)\n",
    "    indices = np.arange(0, total, 1)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices[0:count], indices[count:]\n",
    "\n",
    "dataset = \"titanic\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    #\"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "# Load titanic data\n",
    "path_train = './dataset/titanic/titanic_training.csv'\n",
    "data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "y = data[1:, -1]  # label = survived\n",
    "class_names = [\"Died\", \"Survived\"]\n",
    "labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "y = np.array(y[labeled_idx])\n",
    "y = y.astype(float).astype(int)\n",
    "\n",
    "# Preprocessing dataset\n",
    "X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])\n",
    "X = X[labeled_idx, :]\n",
    "Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "assert X.shape[1] == Z.shape[1]\n",
    "features = list(data[0, :-1]) + onehot_features\n",
    "\n",
    "# Split into training and validation sets.\n",
    "X_titanic = X\n",
    "y_titanic = y\n",
    "Z_titanic = Z\n",
    "titanic_val, titanic_training = shuffle_partition(X_titanic, 150,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "2c0ad3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Spam dataset\n",
    "\n",
    "dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    # \"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "\n",
    "features = [\n",
    "    \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "    \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "    \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "    \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "    \"ampersand\"\n",
    "]\n",
    "assert len(features) == 32\n",
    "\n",
    "# Load spam data\n",
    "path_train = './dataset/spam/spam_data.mat'\n",
    "data = scipy.io.loadmat(path_train)\n",
    "X = data['training_data']\n",
    "y = np.squeeze(data['training_labels'])\n",
    "Z = data['test_data']\n",
    "class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "# Split into training and validation sets.\n",
    "X_spam = X\n",
    "y_spam = y\n",
    "Z_spam = Z\n",
    "spam_val, spam_training = shuffle_partition(X_titanic, 900,5280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "11b0cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree, Titanic, Training 0.6219081272084805\n",
      "Decision Tree, Titanic, Validataion 0.5533333333333333\n",
      "Random Forest, Titanic, Training 0.8091872791519434\n",
      "Random Forest, Titanic, Validation 0.7733333333333333\n",
      "Decision Tree, Spam, Training 0.79337899543379\n",
      "Decision Tree, Spam, Validation 0.8077777777777778\n",
      "Random Forest, Spam, Training 0.8251141552511415\n",
      "Random Forest, Spam, Validation 0.8322222222222222\n"
     ]
    }
   ],
   "source": [
    "# 4.4 Evaluation\n",
    "\n",
    "# Calculates accuracy given predictions and labels.\n",
    "def accuracy(X, y):\n",
    "    total_correct = 0\n",
    "    for i in range(len(X)):\n",
    "        if X[i] == y[i]:\n",
    "            total_correct += 1\n",
    "    return total_correct/len(X)\n",
    "\n",
    "# Train decision tree on titanic training data and predict on validation data.\n",
    "dt_titanic = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_titanic.fit(X_titanic[titanic_training], y[titanic_training])\n",
    "dt_titanic_predictions_training = dt_titanic.predict(X_titanic[titanic_training])\n",
    "print(\"Decision Tree, Titanic, Training\", accuracy(dt_titanic_predictions_training, y_titanic[titanic_training]))\n",
    "dt_titanic_predictions_val = dt_titanic.predict(X_titanic[titanic_val])\n",
    "print(\"Decision Tree, Titanic, Validataion\", accuracy(dt_titanic_predictions_val, y_titanic[titanic_val]))\n",
    "\n",
    "# Train random forest on titanic training data and predict on validation data.\n",
    "rf_titanic = RandomForest(random_states=6, max_depth=3, feature_labels=features)\n",
    "rf_titanic_predictions_training = rf_titanic.fit_and_predict(X_titanic[titanic_training], y_titanic[titanic_training], X_titanic[titanic_training])\n",
    "print(\"Random Forest, Titanic, Training\", accuracy(rf_titanic_predictions_training, y_titanic[titanic_training]))\n",
    "rf_titanic_predictions_val = rf_titanic.fit_and_predict(X_titanic[titanic_training], y_titanic[titanic_training], X_titanic[titanic_val])\n",
    "print(\"Random Forest, Titanic, Validation\", accuracy(rf_titanic_predictions_val, y_titanic[titanic_val]))\n",
    "\n",
    "# Train decision tree on spam training data and predict on validation data.\n",
    "dt_spam = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_spam.fit(X_spam[spam_training], y_spam[spam_training])\n",
    "dt_spam_predictions_training = dt_spam.predict(X_spam[spam_training])\n",
    "print(\"Decision Tree, Spam, Training\", accuracy(dt_spam_predictions_training, y_spam[spam_training]))\n",
    "dt_spam_predictions_val = dt_spam.predict(X_spam[spam_val])\n",
    "print(\"Decision Tree, Spam, Validation\", accuracy(dt_spam_predictions_val, y_spam[spam_val]))\n",
    "\n",
    "# Train random forest on spam training data and predict on validation data.\n",
    "rf_spam = RandomForest(random_states=6, max_depth=5, feature_labels=features)\n",
    "rf_spam_predictions_training = rf_spam.fit_and_predict(X_spam[spam_training], y_spam[spam_training], X_spam[spam_training])\n",
    "print(\"Random Forest, Spam, Training\", accuracy(rf_spam_predictions_training, y_spam[spam_training]))\n",
    "rf_spam_predictions_val = rf_spam.fit_and_predict(X_spam[spam_training], y_spam[spam_training], X_spam[spam_val])\n",
    "print(\"Random Forest, Spam, Validation\", accuracy(rf_spam_predictions_val, y_spam[spam_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "919051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Write data to csv file.\n",
    "\n",
    "def create_file(prediction_file, file_name):\n",
    "    header = ['Id', 'Category']\n",
    "    data_file = []\n",
    "    for i in range(len(prediction_file)):\n",
    "        data_file.append([i+1, int(prediction_file[i])])\n",
    "    with open(file_name, 'w', newline = '') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data_file)\n",
    "    f.close()\n",
    "    \n",
    "# Test predictions for Titanic\n",
    "test_predictions_titanic = rf_titanic.fit_and_predict(X_titanic, y_titanic, Z_titanic)\n",
    "create_file(test_predictions_titanic, \"titanic_predictions.csv\")\n",
    "\n",
    "# Test predictions for SPAM\n",
    "test_predictions_spam = rf_spam.fit_and_predict(X_spam, y_spam, Z_spam)\n",
    "create_file(test_predictions_spam, \"spam_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "a13dc56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "60a7e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Id', 'Category']\n",
    "data = []\n",
    "def create_file(prediction_file, file_name):\n",
    "    for i in range(len(prediction_file)):\n",
    "        data.append([i+1, int(prediction_file[i])])\n",
    "    with open(file_name, 'w', newline = '') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ff6a3405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7271ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
