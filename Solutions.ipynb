{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "1c86706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to install \"gprof2dot\"\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import math\n",
    "\n",
    "import pydot\n",
    "\n",
    "eps = 1e-5  # a small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f998eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 Decision Trees\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    # A method that takes in some feature of the data, the labels and a threshold, and compute \n",
    "    # the information gain of a split using the threshold.\n",
    "    def information_gain(X, y, thresh):\n",
    "        # A method that takes in the labels of data stored at a node and compute the entropy \n",
    "        def gini_impurity(X, y):  \n",
    "            d = dict()\n",
    "            for i in range(len(X)):\n",
    "                if y[i] in d:\n",
    "                    d[y[i]] += 1\n",
    "                else:\n",
    "                    d[y[i]] = 1\n",
    "\n",
    "            entropy = 0\n",
    "            for key in d:\n",
    "                p_c = d[key]/len(X)\n",
    "                entropy -= p_c * math.log2(p_c)\n",
    "            return entropy\n",
    "    \n",
    "        # Compute current entropy\n",
    "        H_S = gini_impurity(X, y)\n",
    "        \n",
    "        # Compute entropy after splitting by threshold.\n",
    "        left_X, left_y = [], []\n",
    "        right_X, right_y = [], []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] < thresh:\n",
    "                left_X.append(X[i])\n",
    "                left_y.append(y[i])\n",
    "            else:\n",
    "                right_X.append(X[i])\n",
    "                right_y.append(y[i])\n",
    "                \n",
    "        # Compute information gain        \n",
    "        H_l = gini_impurity(left_X, left_y)\n",
    "        H_r = gini_impurity(right_X, right_y)\n",
    "        H_after = (H_l * len(left_X) + H_r * len(right_X)) / len(X)\n",
    "        return H_S - H_after\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    # Grows a decision tree by constructing nodes. Using the entropy and segmenter methods, it attempts \n",
    "    # to find a configuration of nodes that best splits the input data. This function figures out the split \n",
    "    # rules that each node should have and figures out when to stop growing the tree and insert a leaf node. \n",
    "    # There are many ways to implement this, but eventually your DecisionTree should store the root node \n",
    "    # of the resulting tree so you can use the tree for classification later on. Since the height of your \n",
    "    # DecisionTree shouldn’t be astronomically large (you may want to cap the height—if you do, the max height \n",
    "    # would be a hyperparameter), this method is best implemented recursively.\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node splits.\n",
    "            \n",
    "            thresh = np.array([np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10) for i in range(X.shape[1])])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "                \n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            \n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    # Given a data point, traverse the tree to find the best label to classify the data point as. Start at \n",
    "    # the root node you stored and evaluate split rules at each node as you traverse until you reach a leaf node, \n",
    "    # then choose that leaf node’s label as your output label.\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1430cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Random Forests\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, random_states, max_depth, feature_labels):\n",
    "        self.random_states = random_states\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_labels = feature_labels\n",
    "        \n",
    "    def fit_and_predict(self, training_X, training_y, test_X):\n",
    "        predictions = [0] * len(test_X)\n",
    "        for i in range(self.random_states):\n",
    "            dt_titanic = DecisionTree(max_depth=3, feature_labels=features)\n",
    "            rand_X = []\n",
    "            rand_y = []\n",
    "            for i in range(len(training_X)):\n",
    "                randIndex = np.random.randint(len(training_X))\n",
    "                rand_X.append(training_X[randIndex])\n",
    "                rand_y.append(training_y[randIndex])\n",
    "            dt = DecisionTree(self.max_depth, self.feature_labels)\n",
    "            dt.fit(np.array(rand_X), np.array(rand_y))\n",
    "            predictions += dt.predict(test_X)\n",
    "        predictions *= 1/self.random_states\n",
    "        return np.around(predictions)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "75a8b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == ''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == '-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "2259b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
    "        print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ba6cd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates accuracy given predictions and labels.\n",
    "def accuracy(X, y):\n",
    "    total_correct = 0\n",
    "    for i in range(len(X)):\n",
    "        if X[i] == y[i]:\n",
    "            total_correct += 1\n",
    "    return total_correct/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "66b65563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns indices for training_labels and training_data that are set aside for validation and for training. Returns\n",
    "# validation and training indices.\n",
    "def shuffle_partition(data, count, total):\n",
    "    np.random.seed(189)\n",
    "    indices = np.arange(0, total, 1)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices[0:count], indices[count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1fa39ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n",
      "Features: ['pclass', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'male', 'female', 'S', 'C', 'Q']\n",
      "Train/test size: (999, 14) (310, 14)\n",
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.6166166166166166\n",
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "Part (c): sklearn's decision tree\n",
      "Cross validation [0.795      0.825      0.805      0.755      0.74371859]\n"
     ]
    }
   ],
   "source": [
    "# Titanic Dataset\n",
    "\n",
    "dataset = \"titanic\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    #\"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "# Load titanic data\n",
    "path_train = './dataset/titanic/titanic_training.csv'\n",
    "data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "y = data[1:, -1]  # label = survived\n",
    "class_names = [\"Died\", \"Survived\"]\n",
    "labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "y = np.array(y[labeled_idx])\n",
    "y = y.astype(float).astype(int)\n",
    "\n",
    "# Preprocessing dataset\n",
    "print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])\n",
    "X = X[labeled_idx, :]\n",
    "Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "assert X.shape[1] == Z.shape[1]\n",
    "features = list(data[0, :-1]) + onehot_features\n",
    "\n",
    "print(\"Features:\", features)\n",
    "print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "print(\"\\n\\nPart 0: constant classifier\")\n",
    "print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "# Split into training and validation sets.\n",
    "X_titanic = X\n",
    "y_titanic = y\n",
    "Z_titanic = Z\n",
    "titanic_val, titanic_training = shuffle_partition(X_titanic, 150,999)\n",
    "\n",
    "# Basic decision tree\n",
    "print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "dt_titanic = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_titanic.fit(X, y)\n",
    "print(\"Predictions\", dt_titanic.predict(Z)[:100])\n",
    "\n",
    "# Random Forest\n",
    "rf_titanic = RandomForest(random_states=6, max_depth=3, feature_labels=features)\n",
    "\n",
    "print(\"\\n\\nPart (c): sklearn's decision tree\")\n",
    "clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf)\n",
    "out = io.StringIO()\n",
    "\n",
    "# You may want to install \"gprof2dot\"\n",
    "sklearn.tree.export_graphviz(\n",
    "    clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "graph = pydot.graph_from_dot_data(out.getvalue())\n",
    "pydot.graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "48006f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size: (5280, 32) (5749, 32)\n",
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.7329545454545454\n",
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "Part (c): sklearn's decision tree\n",
      "Cross validation [0.80681818 0.81912879 0.82481061 0.80965909 0.81628788]\n"
     ]
    }
   ],
   "source": [
    "# Spam dataset\n",
    "\n",
    "dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    # \"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "\n",
    "features = [\n",
    "    \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "    \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "    \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "    \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "    \"ampersand\"\n",
    "]\n",
    "assert len(features) == 32\n",
    "\n",
    "# Load spam data\n",
    "path_train = './dataset/spam/spam_data.mat'\n",
    "data = scipy.io.loadmat(path_train)\n",
    "X = data['training_data']\n",
    "y = np.squeeze(data['training_labels'])\n",
    "Z = data['test_data']\n",
    "class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "   \n",
    "print(\"Features:\", features)\n",
    "print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "print(\"\\n\\nPart 0: constant classifier\")\n",
    "print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "# Split into training and validation sets.\n",
    "X_spam = X\n",
    "y_spam = y\n",
    "Z_spam = Z\n",
    "spam_val, spam_training = shuffle_partition(X_titanic, 900,5280)\n",
    "\n",
    "# Basic decision tree\n",
    "print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "dt_spam = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_spam.fit(X, y)\n",
    "print(\"Predictions\", dt_spam.predict(Z)[:100])\n",
    "\n",
    "# Random forests\n",
    "rf_spam = RandomForest(random_states=6, max_depth=5, feature_labels=features)\n",
    "\n",
    "print(\"\\n\\nPart (c): sklearn's decision tree\")\n",
    "clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)\n",
    "clf.fit(X, y)\n",
    "evaluate(clf)\n",
    "out = io.StringIO()\n",
    "\n",
    "# You may want to install \"gprof2dot\"\n",
    "sklearn.tree.export_graphviz(\n",
    "    clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "graph = pydot.graph_from_dot_data(out.getvalue())\n",
    "pydot.graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)\n",
    "\n",
    "X_spam = X\n",
    "y_spam = y\n",
    "Z_spam = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ff3c0874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5533333333333333\n",
      "0.7466666666666667\n",
      "0.8077777777777778\n",
      "0.8322222222222222\n"
     ]
    }
   ],
   "source": [
    "# 4.4 Evaluation\n",
    "\n",
    "# Train decision tree on titanic training data and predict on validation data.\n",
    "dt_titanic = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_titanic.fit(X_titanic[titanic_training], y[titanic_training])\n",
    "dt_titanic_predictions = dt_titanic.predict(X_titanic[titanic_val])\n",
    "print(accuracy(dt_titanic_predictions, y_titanic[titanic_val]))\n",
    "\n",
    "# Train random forest on titanic training data and predict on validation data.\n",
    "rf_titanic = RandomForest(random_states=6, max_depth=3, feature_labels=features)\n",
    "rf_titanic_predictions = rf_titanic.fit_and_predict(X_titanic[titanic_training], y_titanic[titanic_training], X_titanic[titanic_val])\n",
    "print(accuracy(rf_titanic_predictions, y_titanic[titanic_val]))\n",
    "\n",
    "# Train decision tree on spam training data and predict on validation data.\n",
    "dt_spam = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt_spam.fit(X_spam[spam_training], y_spam[spam_training])\n",
    "dt_spam_predictions = dt_spam.predict(X_spam[spam_val])\n",
    "print(accuracy(dt_spam_predictions, y_spam[spam_val]))\n",
    "\n",
    "# Train random forest on spam training data and predict on validation data.\n",
    "rf_spam = RandomForest(random_states=6, max_depth=5, feature_labels=features)\n",
    "rf_spam_predictions = rf_spam.fit_and_predict(X_spam[spam_training], y_spam[spam_training], X_spam[spam_val])\n",
    "print(accuracy(rf_spam_predictions, y_spam[spam_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d51f6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Id', 'Category']\n",
    "data = []\n",
    "def create_file(prediction_file, file_name):\n",
    "    for i in range(len(prediction_file)):\n",
    "        data.append([i+1, int(prediction_file[i])])\n",
    "    with open(file_name, 'w', newline = '') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "9ae83d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions for Titanic\n",
    "test_predictions = rf_titanic.fit_and_predict(X_titanic, y_titanic, Z_titanic)\n",
    "create_file(test_predictions, \"titanic_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "cbcbcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Id', 'Category']\n",
    "data = []\n",
    "def create_file(prediction_file, file_name):\n",
    "    for i in range(len(prediction_file)):\n",
    "        data.append([i+1, int(prediction_file[i])])\n",
    "    with open(file_name, 'w', newline = '') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ba3ee0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions for SPAM\n",
    "\n",
    "test_predictions = rf_spam.fit_and_predict(X_spam, y_spam, Z_spam)\n",
    "create_file(test_predictions, \"spam_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dcef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
